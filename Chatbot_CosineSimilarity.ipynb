{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"test.csv\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = df['Questions'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_list = df['Answers'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "- **nltk**: This is a library used for natural language processing (NLP). It helps with tasks like text analysis and language modeling.\n",
    "- **numpy**: This is a library used for numerical computations. It helps with handling arrays and performing mathematical operations.\n",
    "- **TfidfVectorizer from sklearn**: This is a tool used to convert text data into numerical values that represent the importance of words in the text.\n",
    "- **cosine_similarity from sklearn**: This is a tool used to measure how similar two pieces of text are by comparing their numerical representations.\n",
    "- `nltk.download('punkt')`: Downloads a tokenizer that can split text into individual sentences or words.\n",
    "- `nltk.download('wordnet')`: Downloads a database of English words that can help with tasks like finding synonyms.\n",
    "- `nltk.download('stopwords')`: Downloads a list of common words (like \"the\", \"and\", \"is\") that are usually ignored in text analysis because they don't carry much meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\14807\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\14807\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\14807\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **WordNetLemmatizer** and **PorterStemmer** are tools from the **NLTK** library used for processing words.\n",
    "- `WordNetLemmatizer` is initialized to help reduce words to their base or dictionary form.\n",
    "- `PorterStemmer` is initialized to reduce words to their root form by removing suffixes.\n",
    "\n",
    "- **stopwords** is a list of common words (like \"the\", \"is\", \"in\") that are often removed in text processing.\n",
    "- **re** is a library for working with regular expressions, which allows us to search and manipulate strings.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def preprocess_with_stopwords(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
    "    tokens = nltk.word_tokenize(text.lower()) # The text is converted to lowercase and split into individual words (tokens).\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')] # This line removes common words (stopwords) from the list of tokens. These words don't usually add significant meaning and can be ignored.\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens] # Each token is processed by the lemmatizer to convert it to its base form. For example, \"running\" becomes \"run\".\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens] # Each lemmatized token is further processed by the stemmer to reduce it to its root form. For example, \"runner\" might become \"run\".\n",
    "    return ' '.join(stemmed_tokens) # The stemmed tokens are joined back together into a single string with spaces in between each word, and this processed text is returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TfidfVectorizer:** This is a tool that converts text data into numbers. Specifically, it uses a method called TF-IDF (Term Frequency-Inverse Document Frequency) which helps in finding the important words in the text.\n",
    "\n",
    "**tokenizer=nltk.word_tokenize:** This tells the vectorizer to use the `nltk.word_tokenize` function to break the text into individual words.\n",
    "\n",
    "- `[preprocess(q) for q in questions_list]`: This part creates a new list where each question in `questions_list` is processed by a function called `preprocess`. This `preprocess` function is to clean and prepare each question for further analysis..\n",
    "- `vectorizer.fit_transform(...)`: This does two things:\n",
    "  - **fit**: It learns the vocabulary and the importance of each word from the processed questions.\n",
    "  - **transform**: It converts the processed questions into a numerical form (a matrix of numbers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14807\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text):\n",
    "    #This line takes the input text and processes it using another function called preprocess_with_stopwords. \n",
    "    #This function likely removes unnecessary words (stopwords) and cleans the text.\n",
    "    processed_text = preprocess_with_stopwords(text)\n",
    "    print(\"processed_text:\", processed_text)\n",
    "    \n",
    "    #This line converts the processed text into a form that the computer can understand better (using something called vectorizer). \n",
    "    #It transforms the text into a list of numbers (a vector).\n",
    "    vectorized_text = vectorizer.transform([processed_text])\n",
    "\n",
    "    #This line compares the vectorized text to other texts stored in X and finds out how similar they are using a method called cosine similarity\n",
    "    similarities = cosine_similarity(vectorized_text, X) \n",
    "    print(\"similarities:\", similarities)\n",
    "\n",
    "    #This line finds the highest similarity score from the list of similarity scores.\n",
    "    max_similarity = np.max(similarities)\n",
    "    print(\"max_similarity:\", max_similarity)\n",
    "    if max_similarity > 0.6:\n",
    "        #This line makes a list of questions from questions_list that have a similarity score greater than 0.6.\n",
    "        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]\n",
    "        print(\"high_similarity_questions:\", high_similarity_questions)\n",
    "\n",
    "        #This block of code finds the answers that correspond to the similar questions and puts them in a list called target_answers.\n",
    "        target_answers = []\n",
    "        for q in high_similarity_questions:\n",
    "            q_index = questions_list.index(q)\n",
    "            target_answers.append(answers_list[q_index])\n",
    "        print(target_answers)\n",
    "\n",
    "        #This line processes and vectorizes the list of similar questions again.\n",
    "        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
    "\n",
    "        #This line processes the input text again\n",
    "        processed_text_with_stopwords = preprocess_with_stopwords(text)\n",
    "        print(\"processed_text_with_stopwords:\", processed_text_with_stopwords)\n",
    "\n",
    "        #This line converts the processed input text into a vector again.\n",
    "        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])\n",
    "\n",
    "        #This line compares the vectorized input text to the vectorized similar questions and finds the similarity scores.\n",
    "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
    "        #This line finds the index of the question with the highest similarity score.\n",
    "        closest = np.argmax(final_similarities)\n",
    "        return target_answers[closest]\n",
    "    else:\n",
    "        return \"Unable to answer this question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: who is tom bradi\n",
      "similarities: [[0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "max_similarity: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Unable to answer this question.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response('Who is tom brady?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_text: what is machin learn\n",
      "similarities: [[0.         0.         0.         0.         0.77627227 0.\n",
      "  0.         0.        ]]\n",
      "max_similarity: 0.7762722680124386\n",
      "high_similarity_questions: ['What is the role of machine learning in data analytics?']\n",
      "['Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.']\n",
      "processed_text_with_stopwords: what is machin learn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\14807\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:523: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Machine learning plays a crucial role in data analytics by enabling the development of algorithms that can automatically learn from data and make predictions or take actions without being explicitly programmed. It is used for tasks such as classification, regression, clustering, and anomaly detection.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response('what is machine learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: What is Data Anlytics\n",
      "Corrected text: What is Data Analytics\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "# Create a spell checker object\n",
    "spell = Speller()\n",
    "\n",
    "# Define the text to be checked\n",
    "text = 'What is Data Anlytics'\n",
    "\n",
    "# Correct the text\n",
    "corrected_text = spell(text)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Corrected text:\", corrected_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
